%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%  
%% Chapter 1: Introduction to heterogeneous computing
%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\def\ArtDir{01.HeteroComp/figures}%

\chapter{Heterogeneity and the future of computing}
\label{chapter:heterogeneity}

How do you boil down over 40 years of computing history into a few short paragraphs?   
Think of a pendulum swinging from one extreme to another.  Very little time is spent at
the point where forces are balanced.  In fact, that point is precisely where the pendulum is
moving the fastest and those struggling to follow the motion of the pendulum must run the
fastest to keep up.

This picture is perhaps the best way to think of the history of computing.  We can start with the
works of Babbage or Turing, but for the sake of brevity, let's start with the modern era of computing
where the microprocessor emerged.  In those days (the late 70's and early 80's) a programmer
needed to understand the low level details of the system.  A professional programmer as a matter
of their normal work-day wrote in low level languages regularly dropping down to assembly code 
when needed.   Programming was a matter of understanding bottlenecks and then working at the lowest 
level to knock them back to get the primitive computer to do what was needed.

As the 80s progressed and the full force of Moore's law kicked in, the game changed.   With
microprocessor performance doubling ever year, a programmer was actually punished for 
working hard to optimize code for a particular system.  The pendulum had swung from
a world where the programmer was almost a hardware engineer to the opposite.  The ability to 
add new features carried the day.  Productivity and portability where the driving concerns and
programmers came to think of them selves as software engineers working to make every more
complex programs correct and full-featured.

In the early 90s with the introduction of out-of-order execution for a microprocessors instructions, the 
pendulum had reached an extremity where the need to write assembly code fell to the side.  Object oriented
programming and other high level concepts ruled the day for software engineers.  Performance was
truly the job of the hardware and we all rode the Mooore's law wagon into a bright future of
ubiquitous computers filling every facet of our lives.

Pendulums, however, keep moving.  In the early 2000's the ability to manage static effects such as 
leakage came to dominate.   Power densities made cooling a major problem.  Moore's law was 
still with us, but it could no longer be used to double performance every couple years.
As Herb Sutter famously pointed out, ``the free lunch'' for software engineers was over. 

This is ancient history at this point.  Multicore chips emerged.  Core counts were small, cache hierarchies were 
simple, and DRAM cost models were relatively flat.   People had to write multithreaded programs.   While OpenMP
was born in the years before multicore chips to serve the needs of programmers working with supercomputers,
the transition to multicore chips pulled OpenMP into the mainstream of programming.  This was great for 
old-timers who still remembered the old-days of hardware-centered programming (that is, the sort of people 
who created OpenMP in the first place), for the masses of programmers, however, these were dark days.
Understanding hardware and using that understanding to write performance code was with us.

We thought the pendulum had swung through the low point in its journey and that language/compiler technology
would take us back to a day where programmers didn't have to spend their days coding to low level features of the
hardware.  We were wrong.  The pendulum swing into hardware-aware programming was just getting started.
First there were the many-core ships with non-uniform cost models that required programmers to understand
where their threads executed and how they lined up with memory.  But then chaos struck the life of programmers with
the advent of heterogeneous computing.

The idea is simple, though the software response is anything but simple.  Hardware specialized to a specific task is
more power-efficient than general purpose hardware.   If your application does computations that look like a graphics pipeline, 
use a GPU.   If you are operating over arrays of high-dimensionality containing dense blocks, use a Tensor Processor Unit.  If
you are coordinating execution over a wide range of storage, networking, and computational elements, use an infrastructure
processing unit. A system composed of heterogenous hardware elements is a system that for a fixed amount of power runs faster.
This is called \emph{heterogenous computing}.   It's not about moving the work from a CPU onto a GPU and then once on the 
GPU, keeping the work there (something we call \emph{offloading}).   It's the old game of organizing a computation 
so the ``whole is greater than the sum of the parts'' by matching each part of your program to the hardware for which it is best suited.

And this is where the pendulum sits today.   Programmers need to understand a great deal about the hardware.  Programmers need to 
know which classes of algorithms map to which types of hardware.  Programmers need to understand how to target one block of 
code to a throughput optimized device (e.g. a GPU) while another block of code asynchronously runs on another device
that may be latency optimized (e.g. a CPU).  The pendulum will swing back.  Some day, the tools programmers use will 
hide those details from programmers.   That day, however, is not today.   And let's face it, when the pendulum swings to the
blissful extreme of programming productivity where hardware is hidden behind layers of abstraction, we know it will someday
swing back again.  That's just how computing seems to work.

In this book, we will explore heterogeneous computing and how your can write programs that exploit heterogeneity 
using OpenMP.  We are in a world where you have to understand hardware and how software maps onto it.   You can 
complain about that state of affairs or accept it and figure out how to move forward.  OpenMP is an explicit API.  You 
tell the compiler what to do through directives and a supporting API, and it does what you say.    In this book we will
explain how all of that works and in the process tell you about the key design patterns used in heterogeneous computing.

This chapter defines our foundation.  We'll talk about hardware at a high level so you understand what you need to map
your algorithms onto.  Then we will cover core ideas behind OpenMP.  Experienced OpenMP programmers should just skim
that part of this chapter, but we want to provide a reminder for people who haven't written much OpenMP code while at the
same time, clearly establish the jargon of OpenMP that the rest of the book depends on.  Then we'll close this chapter with
a taste of what you can do when programming your GPU with OpenMP.   In the multi-core world, people think of OpenMP
as finding key loops and putting a parallel loop directive before that loop.  We close this chapter with the analog to parallel for.  
Its something we call BUD, \emph{the Big Ugly Directive} which for many people will do most of what they need to get started
with programming a GPU from OpenMP.

\section{The basic building blocks of modern computing}

Computer architecture is a fascinating field. If you want to master it, go to the
famous text book we all learned it from (Hennessy and Patterson).  We will simplify and
abstract the problem by thinking in terms of three basic processor building blocks.

\begin{itemize}
\item  the CPU: the multiprocessor and cache coherent memory
\item  the SIMD or Vector Unit: lock-step execution across vector lanes
\item  the GPU:  Index space, kernels, work-items and work-groups
\end{itemize}

It is perhaps a gross oversimplification, but you can to a large degree views modern heterogeneous systems
as the many different ways you can put those three types of building blocks together.  We will consider each 
in turn

\subsection{The CPU}

Introduce what a standard CPU looks like. This is the host processor.  Introduce Cache coherent shared memory machine. SMP model.

\subsection{SIMD and the vector unit}

\subsection{The GPU}
When discussing parallel programming models in general, is it sometimes important to distinguish between concurrent concepts in the programming model and parts of the hardware.
On CPUs for instance, parallel items called perhaps ``threads'' are assigned to run on physical parts of the CPU called perhaps ``cores''.
This correspondence is not necessarily unique, and parallelism expressed in the programming model may be assigned physical components of the processor in a number of ways.

OpenMP does not give sufficient terminology to name the underlying hardware of a given abstract device, and so to help throughout this book, we define an underlying target device using terminology borrowed from another open-standard programming model called OpenCL (from Khronos).
This is no criticism of OpenMP for it is typically an implementation detail on how the parallel concepts in OpenMP might correspond to physical, real-life hardware.
However in our situation, it is helpful to speak in terms regarding expressing parallelism in OpenMP differently to how that might correspond at run-time to the physical hardware.

A target device is constructed with a hierarchical structure of hardware.
The target device is built from a number of Compute Units (CUs)\index{Compute Unit}.
Each Compute Unit is formed from a number of Processing Elements (PEs)\index{Processing Element}.
The Processing Elements themselves are the hardware that can execute the program instructions, and are organised into Compute Units.
Processing Elements can operate independently but there is normally close ties with the other Processing Elements within the Compute Unit.
There is normally no (or little) interaction between Processing Elements in different Compute Units.
In particular, Processing Elements are not allowed to communicate with each other unless they reside in the same Compute Unit.

Each Processing Element may be able to execute vector (SIMD) instructions\index{SIMD}, and so might contain the requisite hardware for this.
The number of data elements the SIMD instructions process is called the width.
We call the location of each data item within the instruction a SIMD lane\index{SIMD lane}.

This three level hierarchy is shown in Figure \ref{figure:target_device_hierarchy}.
The parallel concepts in the OpenMP programming model will be assigned to Compute Units, Processing Elements, and SIMD lanes.

\begin{figure}[t]
\centerline{\includegraphics[width=200pt]{\ArtDir/target_device.pdf}}
\caption{A target device is formed of a number of Compute Units (CU). Each of these Compute Units is formed of a number of Processing Elements (PE), which may be able to execute vector (SIMD) instructions.}
\label{figure:target_device_hierarchy}
\end{figure}

The device memory is shared and available to all of the hierarchy of hardware.
Each Compute Unit might in addition provide a layer of memory available only to those Processing Elements contained within it.
This we will call (as in OpenCL) the local memory\index{local memory}.
The Processing Elements in each Compute Unit can access this memory, however it is not accessible by Processing Elements outside the Compute Unit.
We will address using this local memory in OpenMP in Section~\ref{sec:team_only_memory}.

This book will show how to program a device like this, such as GPU, using OpenMP.
In later chapters, we include case studies to show the abstract device model and the concepts in OpenMP come together and are applied to real hardware.


\section{Why you need OpenMP: a single code-base for heterogeneous hardware}

\section{The structure of this book}
In the following chapters, we will introduce the necessary concepts for programming heterogeneous systems using OpenMP.
Each chapter will be split into two.
The first part of each chapter will walk through the most commonly used parts required for obtaining good performance on target devices; a ``common core'' for OpenMP target if you will.

The second part of the each chapter will go into some specific details less frequently encountered.
These parts form a comprehensive reference for programming target devices using OpenMP.
As such it covers some of the finer details of the specification that are not required in the main.

The first part alone will be sufficient in the majority of cases for learning most of what is required to program GPUs and other devices with OpenMP.
Readers are welcome to read only the first part of each chapter, and come back to the latter parts if or when they need some additional information on the concepts.


\section{Templates for use in formatting the rest of the book.}

Here is how we handle a code fragment embedded in text.
Consider a simple program that adds two vectors, \code{a} and \code{b}.
\begin{verbatim}
      for (i = 0; i < N; i++) { 
         a[i] = a[i] + b[i];
      }
\end{verbatim}  

For longer code fragments, we put the code in a proper figure.  For example, consider figure~\ref{code:vaddSPMD}

\begin{CodeExample}%
{\textbf{SPMD parallel vector add program} -- \small
Create a team of threads and assign one chunk of loop iterations
to each thread.
}%
{code:vaddSPMD}
\begin{lstlisting}
// OpenMP parallel region and SPMD pattern
#pragma omp parallel
{
   int id, i, Nthrds, istart, iend;
   id = omp_get_thread_num();
   Nthrds = omp_get_num_threads(); 
   istart = id * N / Nthrds;
   iend = (id + 1) * N / Nthrds;
   if (id == Nthrds - 1) iend = N; 
   for (i = istart; i < iend; i++) {
      a[i] = a[i] + b[i];
   }
}
\end{lstlisting}
\end{CodeExample} 



When we have specific constructs to introduce, we use a table with marcros for the constructs themselves.  This way we can make 
sure that we use consistent fonts and styles across the entire book.  Take a look at table~\ref{tab:omp_for} for a good example.

\begin{table}[!htbp]
\centering
\caption{\textbf{A basic worksharing-loop construct in C/C++ and Fortran} 
-- \small
The worksharing-loop construct shares the iterations of a loop among
a team of threads.  The loop is called \texttt{for} in C and \texttt{DO} in Fortran.
Fortran is not block structured, so we need an \texttt{END DO} directive.
Optional clauses give the programmer more control
over the loop construct and include \texttt{schedule}, \texttt{reduction}, and 
\texttt{nowait}. We will discuss these clauses later in this chapter.  Additional clauses define storage attributes 
of the variables used in the worksharing-loop.  We will cover those in 
Chapter~\ref{ch:dataEnv}.  
}
\label{tab:omp_for}
\begin{tabular}{|l|} \hline
\ompbcfor \ompclauses \\ 
%\hspace{5mm} \{   \\
\hspace{5mm} for-loop \\      
%\hspace{5mm} \}    \\           
\hline
\ompbfdo \ompclauses  \\ 
%\hspace{5mm} \{   \\
\hspace{5mm} do-loop   \\
%\hspace{5mm} \}   \\ 
\ompbfdoend \textit{ [nowait] } \\   
                  
\hline

\end{tabular}
\end{table}

Here is an index entry:  Moore's law\index{Moore's law} and a citation Dennard scaling\index{Dennard scaling}~\cite{Dennard}, 


There are some open questions concerning what we need to cover.
\begin{itemize}
\item Do we need to include \code{declare simd}?  Is that enabled for a GPU?  
\item Do we need to discuss the variant directives?  If they are 
important for GPU programmers, then we probably need to. Likewise for assumes and assume.  
\item Does the scan directive apply to GPUs?  
\item Does tile and unroll apply to GPUs?  
\item Do memory spaces apply to GPUs?
\end{itemize}

We will also need to cover a set of internal control variables and where appropriate a set of environment variables.

\begin{table}[h!]
\centering
\caption{All the pragmas we will cover in the book and the chapters where we will cover them}
\label{YourLabel}
\input{\ArtDir/ompGPUpragmas.tex}
 \end{table}

\begin{table}[h!]
\centering
\caption{All the clauses we will cover in the book and the chapters where we will cover them}
\label{YourLabel}
\input{\ArtDir/ompGPUclauses.tex}
 \end{table}
 
\begin{table}[h!]
\centering
\caption{All the runtime functions we will cover in the book and the chapters where we will cover them}
\label{YourLabel}
\input{\ArtDir/ompGPUfuncs.tex}
 \end{table}
 
\begin{table}[h!]
\centering
\caption{All the internal control variables and associated environment variables we will 
cover in the book and the chapters where we will cover them}
\label{YourLabel}
\input{\ArtDir/ompGPUicvEnvVar.tex}
 \end{table}


%-----------------------------------------------------------------------
%------------------------- From Next Step ------------------------------
%-----------------------------------------------------------------------
\section{From The Next Step Chapter 5}

Specialized accelerator processors, which dramatically improve the performance
of some computations, are proliferating, and general-purpose processors are
now very often connected to some type of accelerator.  
The popularity of these heterogeneous architectures across all types of
computing has had a noticeable impact on the development of software.  

To exploit these systems, developers must write software that executes various
regions of code on different types of devices.  There are many reasons for
wanting to do this but very often the motivation is to accelerate
computationally intensive loop nests.  

However, the programming models for these systems are difficult to use.
Often code modules are written twice, once for the general-purpose
processor and then again for the accelerator.  The accelerator version is often
written in a lower-level, accelerator-specific language.  The result is the
undesirable software maintenance problem of keeping two versions of code, which
implement the same algorithm, synchronized.

The \OMP\ Language Committee recognized the need to make it easier to program
heterogeneous architectures and set about to extend \OMP\ to support these types
of systems \cite{Beyer2011}.  The results of this work were initially released
in \OMPfourzero\ and updated in \OMPfourfive.

Software developers can use \OMP\ to program accelerators in a higher-level
language and maintain one version of their code, which can run on either an
accelerator or a general-purpose processor.  In this chapter, we present the
syntax for and describe how to use the \OMP\ \emph{device constructs} and
related runtime functions that were added to support heterogeneous
architectures. 

%What was needed is a programming model that could...
%Given the popularity of heterogeneous systems across all types of computing,

%There is a place here for \OMP\ to make these software packages more
%maintainable and portable.  We hope that in the future, as the latest versions
%of \OMP\ are implemented in more compilers, software developers will leverage
%the \OMP\ device constructs and develop one version of their code that can run
%anywhere, including on accelerators.

%There is a place here for \OMP\ to make these software packages more
%maintainable and portable.  We hope that in the future, as the latest versions
%of \OMP\ are implemented in more compilers, 
