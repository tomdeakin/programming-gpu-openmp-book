%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%  
%% chapter 6: Advanced topics: async and multi-gpu
%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\def\ArtDir{06.MultiGPU/figures}

\chapter{Multi-GPU programming}
\label{chapter:multi_gpu}

Heterogeneity is here! In the previous chapters you've learnt how to program one
of these devices. But as we alluded to in Chapter~\ref{chapter:heterogeneity},
modern processors and systems often contain multiple devices. OpenMP gives us
the tools to program multiple devices within the same program.

Before we cover this topic though, it is worth exploring an alternative view of
systems with multiple devices. Many of the fastest supercomputers in the
world\footnote{As ranked by the Top500 list at \url{www.top500.org}} are
designed by GPU-accelerated nodes connected together with a high-speed
interconnect. Each node contains multiple high-performance GPUs; four or six
GPUs per node is currently a common configuration.

Memory is said to be distributed, as it is not shared between those nodes. Data
is passed between processes running on the nodes via a message passing API;
most commonly this is the standard Message Passing Interface (MPI).

How then, might we map our application to such a system? As an example, consider
a grid-based code, where each cell in the grid can be updated concurrently.
Typically they require some data from neighbouring cells.
To run this code in parallel on a system, we would first perform a domain
decomposition to partition the grid. We would assign each partition to a process
running on a different node in our supercomputer. If the node contained
multi-core CPUs, we could either decompose until there was one process per
physical core, or one processes per node, or some combination in between.
OpenMP could then be used (as in Chapter\ref{chapter:overview}) to thread each
processes and utilise any remaining cores. The worksharing constructs in OpenMP
make this step simple: loops are annotated with a \Code{parallel for} directive
and the work is shared between all cores.

To distribute our work across a system with multiple GPUs per node, we face a
similar choice. The most common approach we find is to assign one processes in
our distributed memory scheme to each GPUs. In a node with four GPUs, we would
run four MPI processes per node. The application therefore would use OpenMP to
target a single GPU, and communication between processes (and GPUs) would occur
firstly via the host and then via the message passing interface. This means that
without advanced and compatible libraries, communication cannot always take
advantage of a network with directly connects local GPUs together.

This is simple, as we only need perform the decomposition of concurrent work
once (for the distributed memory). Each process also needs only be aware of a
single target device. Environment variables and the system queue are used to
ensure processes are distributed across the machine, and each process has
exclusive access to a single GPU.

OpenMP gives us the ability to target multiple accelerators within a single
OpenMP program. This functionality is enabled by the \Code{device} clause, which
can be put on nearly all of the \Code{target} directives. As these directives
can be made asynchronous via the tasking model in OpenMP, the program is able to
run different target tasks in parallel across multiple target devices.

However, the distribution of work is left up to the programmer. There are no
worksharing constructs to generate target tasks automatically so that each
device can be kept busy. Therefore, the programmer must implement another level
of decomposition, to further decompose the partition across the local system.
Although this means communication between GPUs may be fast as it can take
advantage of a device-to-device interconnect path that does not require the
host, it is at the expense of increased code complexity via this manual
worksharing.
This unattractive extra level of decomposition usually results in HPC programs
using just one device per MPI process, using a single level decomposition.

In other domains of course, where distribution of the problem is less common,
the ability to target multiple devices within the same program is very valuable.
This chapter will look at how to write OpenMP programs that can run on multiple
target devices in parallel. We will first look at the target task model so that
target regions can be run asynchronously to the host (and each other), and how
to define the dependencies between them.
We will then show how this can be combined with the \Code{device} clause so that
different tasks can be run on different devices in parallel.

\section{OpenMP tasking model and target regions as tasks}
\begin{itemize}
  \item Very brief introduction to tasking.
  \item Target region is a task.
  \item Initial thread on host.
  \item Mergable included target task.
  \item Initial thread on device, encounters the parallelism.
  \item Undeferred, runs straight away.
\end{itemize}

\section{no wait clause for asynchronous offload}
\begin{itemize}
  \item nowait clause
  \item host continues execution
  \item waiting for tasks with taskwait
  \item simple depends clause
  \item Read Using OpenMP book.
\end{itemize}

\section{in reduction clause}
\label{sec:in_reduction}
\begin{itemize}
  \item task reductions.
  \item might be covered in Chapter~\ref{chapter:memory}\dots
\end{itemize}


\section{Example: TBA}
\begin{itemize}
  \item Need something strong.
  \item File IO?
  \item MPI comms?
\end{itemize}



\section{Further misc topics}
\subsection{if clause}
\label{sec:if_clause}
\begin{itemize}
  \item conditional offload
\end{itemize}

%-----------------------------------------------------------------------
%------------------------- From Next Step ------------------------------
%-----------------------------------------------------------------------
\section{From The Next Step Chapter 6}

\subsection{The Nowait Clause on Device Constructs}
\label{sec:06.async-exec}
\index{Tasking!Depend clause}
\index{Accelerators!Depend clause}
\index{OpenMP clauses!depend}
\index{Accelerators!Nowait clause}
\index{OpenMP clauses!nowait}

%\index{Tasking}
%\index{Task dependences}
%\index{Tasking!Taskwait construct}
%\index{Tasking!Task synchronization construct}
%\index{Data environment with tasking}
%\index{Definition of a task}
%\index{Creation of tasks}
%\index{Synchronization of tasks}
%\index{Scheduling of tasks}
%\index{Tasking!Creation}
%\index{Tasking!Synchronization}
%\index{Tasking!Scheduling}
%\index{Creation of tasks}
%\index{Synchronization of tasks}
%\index{Scheduling of tasks}
%\index{Tasking!Deferred execution}
%\index{Tasking!Task synchronization construct}
%\index{Tasking!Barrier} 
%\index{Tasking!Included task}
%\index{Included task}
%\index{Tasking!Tied task}
%\index{Tied task}
%\index{Tasking!Task scheduling points}
%\index{Task scheduling points}

\index{Tasking!Creation}
\index{Creation of tasks}
% Ruud - Changed to uppercase
\index{Accelerators!Target task}
\index{Accelerators!Device constructs}
To execute code in parallel on both the host and an
accelerator the host thread must not be blocked waiting for a
device construct to finish.  Section~\ref{ssec:06.target-task} discussed the
target task, which is an explicit task that is generated by a \code{target},
\code{target enter data}, \code{target exit data} or \code{target update}
construct.  Because these constructs generate a task, the parallel execution
features of \OMP\ tasking, covered in detail in Chapter~\ref{chap:tasking}, are
now available to the device constructs.

\index{Tasking!Included task}
\index{Tasking!Deferred execution}
By default the target task is included and executed immediately.  The host
thread that generated the target task cannot continue executing the code after
the device construct until the target task completes.  The \code{nowait} clause
changes the target task into a deferrable task.  This means that after
generating the target task, the host thread may immediately continue executing
the code after the device construct.  An explicit parallel region is not
required to execute a host task in parallel with a target task.  A single host
thread may execute a host task, while in parallel, the device construct is executed in
by an accelerator.  The code example in
Figure~\ref{figure:chapter6-nowait-v2} uses the \code{nowait} clause to allow the
host thread to continue past the \code{target} construct at lines $7-9$.  A
target task is generated that encloses the execution of the target region.

\begin{figure*}[!tb]
\begin{verbatim}
 1 extern int max(int,int);
 2 #pragma omp declare target(max)
 3 void F(char *v, short *restrict s, int n)
 4 {
 5   int i;
 6 
 7   #pragma omp target nowait map(v[0:n])
 8   for (i=0; i<n; i++)
 9     v[i] = max(v[i],0);
10 
11   for (i=0; i<n; i++)
12     s[i] = max(s[i],0);
13   #pragma omp taskwait
14 
15   for (i=0; i<n; i++)
16     s[i] = s[i] - v[i];
17 }
\end{verbatim}
\caption{ \textbf {Example using the nowait clause } -- \small
          Execute the target region on an accelerator in 
          parallel with the code executing on the host.
         }
\label{figure:chapter6-nowait-v2}
\end{figure*}

\index{Tasking!Taskwait construct}
A thread on the accelerator executes the target region.  In parallel, the host
thread executes the loop at lines $11-12$.  It then encounters the
\code{taskwait} construct at line $13$ and waits there until the target task
generated at line $7$ is complete.  The host thread then executes the
remainder of the function.

\index{Tasking!Scheduling}
\index{Scheduling of tasks}
Similar to the \code{task} construct, the \code{depend} clause may be
used to express target task dependences.  If a \code{depend} clause appears on
a device construct then the generated target task cannot be scheduled to
execute until the dependences in the clause are satisfied.

\index{Tasking!Data environment}
\index{Data environment with tasking}
A target task's \code{private} and \code{firstprivate} variables are created and initialized
when the task is generated.  However, map-enter phase assignments for mapped
variables occur when the target task executes, and map-exit phase assignments
occur when the task completes. 

\index{Task dependences}
Tasks may be used to overlap computation with data transfers between the host
and accelerator.  There can be dependences between target tasks and other tasks
generated by the \code{task} constructs.  In
Figure~\ref{figure:chapter6-nowait-depend}, the \code{nowait} and
\code{depend} clauses are used to execute target tasks in parallel with other tasks.

\begin{figure*}[!tb]
\begin{verbatim}
 1 extern void h0(int*, int);
 2 extern void h1(int*, int);
 3 extern void t1(int*, int*, int);
 4 #pragma omp declare target(t1)
 5 void F(int *a, int *b, int n)
 6 {
 7   #pragma omp parallel num_threads(2)
 8   #pragma omp single
 9   {
10   #pragma omp target enter data map(to:a[:n]) \
11               nowait depend(out:a[:n]) // t0
12 
13   #pragma omp task depend(out:b[:n])
14   h0(b, n);
15 
16   #pragma omp target map(to:b[:n]) \
17               nowait depend(in:b[:n]) depend(inout:a[:n])
18   t1(a, b, n);
19 
20   #pragma omp task depend(in:b[:n])
21   h1(b,n);
22 
23   #pragma omp target exit data map(from:a[:n]) \
24               depend(in:a[:n]) // t2
25   }
26 }
\end{verbatim}
\caption{ \textbf {Example using the nowait and depend clauses } -- \small
          Use the \texttt{depend} and \texttt{nowait} clauses to execute target
          tasks in parallel with other host tasks.
         }
\label{figure:chapter6-nowait-depend}
\end{figure*}

\index{OpenMP constructs!Single}
At line $7$ a parallel region is started with two threads.  Because of the
\code{single} construct, one host thread executes the region and generates a
sequence of tasks.  
In the following discussion, the tasks are labeled ($task name$), when
the are generated.

A deferrable target task ($t0$) is generated for the
\code{target}~\code{enter}~\code{data} construct at line $10$.
When it executes, the $t0$ task performs a map-enter for the memory pointed to by \code{a}.  
A host task ($h0$) is generated that encloses the call to the function \code{h0()} at line $14$.
The $h0$ host task and the $t0$ target task may execute in parallel.
A deferrable target task ($t1$) is generated that encloses the call to the function \code{t1()} at line $18$.
Because of the task dependences expressed in the \code{depend} clauses at line
$17$, the $t1$ target task cannot be scheduled to execute until the $t0$ and
$h0$ tasks have completed.
A host task ($h1$) is generated that encloses the call to the function \code{h1()} at line $21$.
The $h1$ task cannot be scheduled to
execute until the dependence on \code{b[:n]} is satisfied, which occurs when the
$h0$ task completes.  
Notice that the $t1$ and $h1$ tasks may execute in
parallel and that the $h1$ task may start before $t1$.  
% Ruud - Changed to uppercase
\index{Accelerators!Target task}
\index{Tasking!Depend clause}
\index{Accelerators!Depend clause}

An included target task ($t2$) is generated for the \code{target exit data}
construct at line $23$, and when it executes, it performs a map-exit phase for
the memory pointed to by \code{a}.  
A \code{nowait} clause does not appear on the construct, but there is a
\code{depend} clause.  The target task $t2$ cannot be scheduled to execute until the dependence
on \code{a[:n]} is satisfied by the completion of the $t1$ task.  
The host task (and thus the host thread)
that generated $t2$ cannot be resume execution until $t2$ completes.
Finally, there is a task
synchronization point at the implicit barrier at line $25$.

%-----------------------------------------------------------------------
%------------------------- New section ---------------------------------
%-----------------------------------------------------------------------
\subsection{Selecting a Device}
\label{sec:06.which-device}
\index{Accelerators!Multiple accelerators} 
\index{Accelerators!Device}

\OMP\ provides support for multiple accelerators.  Devices are
enumerated such that each one has a unique device number.  The actual number
for a specific accelerator is implementation-defined.  A device number
for an accelerator must be non-negative and less than the value
returned by the \code{omp_get_num_devices()} runtime function.
\index{Accelerators!omp\_get\_num\_devices} 
\index{OpenMP runtime functions!omp\_get\_num\_devices} 

There is currently no support in \OMP\ to determine the characteristics of a
particular accelerator device.  The only distinction made between devices is
that there is a host device where the program begins execution and an optional
set of accelerator devices.

\index{Accelerators!omp\_get\_initial\_device} 
\index{OpenMP runtime functions!omp\_get\_initial\_device} 
You can find the device number for the host device using the
\code{omp_get_initial_device} routine.  The host device number has some odd
restrictions.  If it is not greater than or equal to zero and less than the
value returned by the \code{omp_get_num_devices} runtime function, then it may
be used only in certain device memory runtime functions (see
Section~\ref{sec:06.device-memory-routines}).

\index{Accelerators!Device constructs}
\index{Accelerators!Host fall back} 
Programs using the \OMP\ device constructs should be be portable,
to systems that do not have accelerators.  \OMP\ supports \emph{host
fall back}, which is the concept that a target region can always be executed by
the host device.  If you take an \OMP\ program that contains device constructs
and compile and run that program on a system without accelerators, then the
device constructs are executed on the host device.

%-----------------------------------------------------------------------
%------------------------- New subsection ------------------------------
%-----------------------------------------------------------------------
\subsubsection{The Default Device and the Device Clause}
\label{ssec:06.device-clause}
\index{Accelerators!Device clause}
\index{OpenMP clauses!device}
\index{Accelerators!Device}
\index{Accelerators!Default device}

The device number determines to which device a construct applies.  The
\code{device} clause is used to specify a device number.  When there is no
\code{device} clause, the device number for a \code{target}, \code{target
data}, \code{target update}, \code{target enter data} or \code{target exit
data} construct is the \emph{default-device-var} ICV.  There may be only one
\code{device} clause on a construct, and the expression in the clause must be
non-negative.  If the device number specified for a device construct does not
correspond to any devices in the system where the program is running, then the
construct falls back to the host device.
% Ruud - Changed to a "D" for consistency.
\index{Accelerators!Default-device-var ICV}
\index{ICV!default-device-var}
\index{Accelerators!Device constructs}

% Ruud -Does not fit : \index{Accelerators!OMP\_DEFAULT\_DEVICE environment variable} 
\index{Accelerators!OMP\_DEFAULT\_DEVICE} 
\index{OpenMP environment variables!OMP\_DEFAULT\_DEVICE} 
When a program begins execution, the \emph{default-device-var} ICV is
initialized to the value of the \code{OMP_DEFAULT_DEVICE} environment variable.
If the environment variable is not set, then the \emph{default-device-var} is
implementation-defined.  During program execution you may determine the default
device using the \code{omp_get_default_device} function or change the default
device using the \code{omp_set_default_device} function.
\index{Accelerators!omp\_get\_default\_device} 
\index{OpenMP runtime functions!omp\_get\_default\_device} 
\index{Accelerators!omp\_set\_default\_device} 
\index{OpenMP runtime functions!omp\_set\_default\_device} 

An example using the \code{device} clause and related runtime functions is
shown in Figure~\ref{figure:chapter6-device}. The example first copies the value of the
array \code{a} from the host to all of the accelerators and then starts a
target region on all devices.  Tasks are used to
execute the operations in parallel.

\begin{figure*}[!tb]
\begin{verbatim}
 1 #include <omp.h>
 2 extern int a[1024]; int Work(int *, int, int);
 3 #pragma omp declare target to(a, Work)
 4 
 5 void F()
 6 {
 7   int defdev = omp_get_default_device();
 8   int numdev = omp_get_num_devices();
 9 
10   for (int i=0; i<numdev; i++) {
11     omp_set_default_device(i);
12     #pragma omp target update to(a) nowait
13   }
14   omp_set_default_device(defdev);
15   #pragma omp taskwait
16 
17   for (int i=0; i<numdev; i++) {
18     #pragma omp target device(i) nowait
19     Work(a,i,numdev);
20   }
21 
22   if (numdev == 0) Work(a,0,1);
23   #pragma omp taskwait
24 }
\end{verbatim}
\caption{ \textbf {Example of the device clause and related runtime functions} -- \small
          The variable \texttt{a} is updated with the host's value on all devices
          and then the function \texttt{Work()} is executed by all devices.
         }
\label{figure:chapter6-device}
\end{figure*}

The assignments on lines $7-8$ use runtime functions to find the current
default device and the number of devices.  Each time through the loop spanning
lines $10-13$, the default device number is changed. The
\code{target}~\code{update} construct makes the variable \code{a} consistent between
the host and the default device using the value of \code{a} from the host. Because
of the \code{nowait} clause, the target update executes as a deferrable target
task (see Section~\ref{sec:06.async-exec}).  The host thread does not wait for
the target update region to complete.  

% Ruud - Changed to uppercase
\index{Accelerators!Target task} 
\index{OpenMP clauses!taskwait}
The default device is restored at line $14$.  The host thread waits at the
\code{taskwait} at line $15$ until all of the tasks started at line $12$ have
finished.  In the loop spanning lines $17-20$, the \code{device} clause is used
to select a specific device where the target region executes.  Again, because
of the \code{nowait} clause a deferrable target task is generated that encloses
the target region.  The host thread executing the loop does not wait for the
target region to complete before continuing.  The conditional call to the
function \code{Work()} at line $22$ is there just in case \code{omp_get_num_devices()}
returns 0.  The \code{taskwait} at line $23$ ensures that all of the tasks
started at line $18$ have completed before returning from the function.
\index{Accelerators!omp\_get\_num\_devices} 
\index{OpenMP runtime functions!omp\_get\_num\_devices} 
\index{Accelerators!Nowait clause} 
\index{OpenMP clauses!nowait}

%-----------------------------------------------------------------------
%------------------------- New subsection ------------------------------
%-----------------------------------------------------------------------
\subsubsection{The If Clause on Device Constructs}
\label{ssec:06.if-clause}
\index{Accelerators!If clause}
\index{OpenMP clauses!if}
\index{Accelerators!Device constructs}

An \code{if} clause
may appear on a \code{target}, \code{target data}, \code{target update},
\code{target enter data} or \code{target exit data} construct.  

\index{Accelerators!Host fall back} 
Except for the \code{target update} construct, when the expression in an
\code{if} clause evaluates to \emph{false}, then host fall back occurs and: 
\begin{itemize} 
\item The device for the construct is the host.  
\item The execution of the region occurs on the host device.
\item Variables appearing in \code{map} clauses are mapped to the host's device data environment.
\item If a \code{device} clause appears on the construct, it is ignored. 
\end{itemize}

When the \code{if} clause expression evaluates to \emph{false} on a
\code{target update} construct, then assignments resulting from the construct
do not occur.

The \code{if}
clause is useful for setting a threshold on the amount of computation in a
target region versus the expected overhead of launching the target region on an
accelerator device.  A simple code example using the \code{if} clause is shown
in Figure~\ref{figure:chapter6-if}.

\begin{figure*}[!tb]
\begin{verbatim}
1 #define MB (1024*1024)
2 extern void Work(float*, int);
3 #pragma omp declare target(Work)
4 void F(float * restrict a, int n)
5 {
6    #pragma omp target if(n > MB) map(a[:n])
7    Work(a,n);
8 }
\end{verbatim}
\caption{ \textbf {Example of an if clause on the target construct} -- \small
          If \texttt{n} is greater than a threshold, execute the target region on the
          default accelerator.  Otherwise, execute the region on the host device.
         }
\label{figure:chapter6-if}
\end{figure*}

Be careful when using the \code{if} clause on constructs that
map variables or effect the value of mapped variables.  
For example, if an \code{if} clause evaluates to false on a
\code{target update} construct and a \code{target} construct is dependent on
the execution of the target update, then a program error may occur.

%-----------------------------------------------------------------------
%------------------------- New section ---------------------------------
%-----------------------------------------------------------------------
\subsection{The Device Pointer Clauses}
\label{sec:06.Device-pointer-clauses}
\index{Accelerators!Device}
\index{Accelerators!Device pointer}
\index{Device pointer}

The clauses described in this section are used to refer to device pointers.
Recall from Section~\ref{ssec:06.device-pointers} that a device pointer is a
pointer variable on the host whose value is an object that represents an
address in device memory.

Assignments to a device pointer are restricted to values that arise
from the following cases:

\begin{itemize}

  \item The return value of the \code{omp_target_alloc()} function.
  \index{Accelerators!omp\_target\_alloc function} 
  \index{OpenMP runtime functions!omp\_target\_alloc} 

  \item The address of a variable referenced in the lexical scope of a
  \code{target data} construct when that variable appeared in a
  \code{use_device_ptr} clause on the construct.

  \item The return value of an implementation-defined device memory allocation
  function.\footnote{For example, CUDA's \code{cudaAlloc}\cite{CUDA-website} or OpenCL's
  \code{BufferAllocate}\cite{OpenCL-website} functions.}

  %An \OMP\ implementation may provide support for other
  %memory allocation  functions that are similar to \code{omp_target_alloc} 

\end{itemize}

The operations on a device pointer variable on the host are restricted as
follows:

\begin{itemize}
  \item  A device pointer variable cannot be de-referenced.
  \item  Pointer arithmetic cannot be performed on a device pointer variable.
\end{itemize}

%-----------------------------------------------------------------------
%------------------------- New subsection ------------------------------
%-----------------------------------------------------------------------
\subsubsection{The Is\_device\_ptr Clause}
\label{ssec:06.is_device_ptr-clause}
\index{Accelerators!Is\_device\_ptr clause}
\index{OpenMP clauses!use\_device\_ptr}

The purpose of the \code{is_device_ptr} clause is to provide a way for a device
pointer to be accessed in a target region.
The \code{is_device_ptr} clause accepts a list of device pointer variable names
and may only appear on a \code{target} construct.

When a device pointer appears in an \code{is_device_ptr} clause on a
\code{target} construct, the corresponding variable in the region is private.
On entry to the target region, the corresponding private variable is
initialized with the device's representation of the address stored in the
original device pointer.

A simple example of a \code{target} construct with an \code{is_device_ptr}
clause is shown in Figure~\ref{figure:chapter6-isptr}.  In line $4$, a device
address on the \code{dev} device is assigned to \code{dptr}.  The device pointer \code{dptr}
is listed in the \code{is_device_ptr} clause at line $6$ indicating that \code{dptr}
is private in the target region.  On entry to the target region, the private instance
of \code{dptr} is initialized with the \code{dev} device's representation of the device
address that corresponds to the variable's original value.

\begin{figure*}[!tb]
\begin{verbatim}
 1 #include <omp.h>
 2 void *init(int n, int dev)
 3 {
 4   char *dptr = omp_target_alloc(dev, n);
 5 
 6   #pragma omp target is_device_ptr(dptr) device(dev) 
 7   for (int i=0; i<n; i++)
 8     dptr[i] = i;
 9 
10   return (void*)dptr;
11 }
\end{verbatim}
\caption{ \textbf {Example of the is\_device\_ptr clause} -- \small
          The device pointer variable \texttt{dptr} must appear in
          the \texttt{is\_device\_ptr} clause to 
          de-reference it in the target region.
        }
\label{figure:chapter6-isptr}
\end{figure*}

%-----------------------------------------------------------------------
%------------------------- New subsection ------------------------------
%-----------------------------------------------------------------------
\subsubsection{The Use\_device\_ptr Clause}
\label{ssec:06.use_device_ptr-clause}
\index{Accelerators!Use\_device\_ptr clause}
\index{OpenMP clauses!use\_device\_ptr}

The purpose of the \code{use_device_pointer} clause is
to provide a way to refer to the device address of a mapped variable,
so that it may be passed to a function as a device pointer argument.
Some vendors implement optimized functions that expect device pointers as
arguments.  These optimized functions, called by a host thread, offload their
execution to an accelerator, either using a \code{target} construct with an
\code{is_device_ptr} clause or some other vendor-specific mechanism.

The \code{use_device_ptr} clause accepts a list of variable names.  The clause
valid only on a \code{target data} construct.  The clause applies to the
lexical scope of its associated \code{target data} construct.  In
the construct, references to variables that appear in a \code{use_device_ptr} clause must
be to the address of the variable.  In the lexical scope of the
\code{target}~\code{data} construct, a reference to the address of a variable
that appears in a \code{use_device_ptr} clause is replaced with the
corresponding device address.

In Figure~\ref{figure:chapter6-useptr}, the variable \code{A} is globally mapped by
the \code{declare}~\code{target} construct at line $2$.  Because of the
\code{use_device_ptr} clause on the \code{target}~\code{data} construct at line
$8$, the reference to the address of \code{A} in the call to \code{AccelFunc()} at line
$9$ is replaced with the device address of \code{A}.

\begin{figure*}[!tb]
\begin{verbatim}
 1 int A[1024];
 2 #pragma omp declare target to(A)
 3 extern int AccelFunc(void *);
 4 
 5 int Func()
 6 {
 7   int err;
 8   #pragma omp target data map(err) use_device_ptr(A)
 9   err = AccelFunc(A); // Requires device address of A
10   return err;
11 }
\end{verbatim}
\caption{ \textbf {Example of the use\_device\_ptr clause} -- \small
          Replace the reference to the host address of \texttt{A} in the lexical
          scope of the \texttt{target data} construct with
          the device address of \texttt{A}.
        }
\label{figure:chapter6-useptr}
\end{figure*}

%-----------------------------------------------------------------------
%------------------------- New section ---------------------------------
%-----------------------------------------------------------------------
\subsection{Device Memory Functions}
\label{sec:06.device-memory-routines}

\index{Accelerators!Device}
\index{Accelerators!Device pointer}
\index{Device pointer}
\OMP\ provides a set of runtime functions for creating, copying, and mapping
dynamically allocated device memory.  They use device pointers (see
Section~\ref{ssec:06.device-pointers} on page~\pageref{ssec:06.device-pointers}) and provide advanced users with
capabilities to share complex data structures across devices.  
These device memory functions are described in detail in
Section~\ref{ssec:02.new_runtime_functions_3} starting on
page~\pageref{ssec:02.new_runtime_functions_3}.  
In this section, some simple examples using the functions are presented and discussed.

%The \code{omp_target_alloc()} and \code{omp_target_free()} functions allocate and
%free device memory.  The \code{omp_target_alloc()} function returns a device
%pointer and the \code{omp_target_free} routine takes a device pointer as an
%argument.  The \code{omp_target_memcpy()} and \code{omp_target_memcpy_rect()}
%functions copy memory from one device to another.  

%The \code{omp_target_alloc()} function dynamically allocates 
%device memory and returns a device pointer.
%The \code{omp_target_memcpy()} 
%function copies memory from a source device to a destination device.

\paragraph{Copy a Linked List To Device Memory}

The example code in Figure~\ref{figure:chapter6-alloc} uses 
the \code{omp_target_memcpy()} function to
copy a linked list from the host to the default device.  Storage for the linked list is allocated
in the default device's memory using the \code{omp_target_alloc()} function.
\index{Accelerators!omp\_target\_alloc} 
\index{OpenMP runtime functions!omp\_target\_alloc} 
\index{Accelerators!omp\_target\_memcpy} 
\index{OpenMP runtime functions!omp\_target\_memcpy} 

\begin{figure*}[!tb]
\begin{verbatim}
 1 #include <omp.h>
 2 #include <stdlib.h>
 3 typedef struct item {struct item *next; int v; } item_t;
 4 void *copy_list2dev(item_t *list)
 5 {
 6   int i, count=0;
 7   int dev  = omp_get_default_device();
 8   int host = omp_get_initial_device();
 9   item_t *src = NULL, *dst = NULL;
10 
11   if (list == NULL) return NULL;
12   for (src=list; src; src=src->next)
13     count++;
14   dst = omp_target_alloc(count*sizeof(item_t), dev);
15   
16   for (src=list, i=0; src; src=src->next, i++)
17     omp_target_memcpy(dst, src, sizeof(item_t),
18                       i*sizeof(item_t), 0,
19                       dev, host);
20                       
21   #pragma omp target is_device_ptr(dst)
22   {
23     for (i=0; i<count-1; i++)
24       dst[i].next = &dst[i+1];
25     dst[i].next = NULL;
26   } 
27   return (void*)dst;
28 } 
\end{verbatim}
\caption{ \textbf {Copy a linked list to device memory} -- \small
          Copy a linked list from the host to dynamically allocated
          device memory.
         }
\label{figure:chapter6-alloc}
\end{figure*}

After counting the number of elements in the list, the \code{omp_target_alloc()}
function is called at line $14$ to allocate memory for the linked list in the
accelerator's address space.   The variable \code{dst} is assigned the device
pointer returned by \code{omp_target_alloc()}.  In lines $16-19$, each list item
is copied from the host to the device using the \code{omp_target_memcpy()}
function.  The destination device is the default device, and the source device
is the host.

A host thread cannot perform pointer arithmetic on an accelerator's device
pointer.  Fortunately, the \code{omp_target_mempcy()} function has offset
arguments for the source and destination addresses.  The destination
accelerator address is calculated by adding the \code{i*sizeof(item_t)} offset
expression to the \code{dst} device pointer.

Finally, in lines $21-26$, the \code{next} pointers in the copied list items are
initialized by running a short target region on the destination device.  The
\code{is_device_ptr} clause indicates that \code{dst} is a device pointer in the
target region.
\index{Accelerators!Is\_device\_ptr clause}
\index{OpenMP clauses!is\_device\_ptr}

\paragraph{Associate Host Memory with Device Memory}

\index{Accelerators!Device data environment}
\index{Device data environment}
\index{Device data environment!Present in}
Host memory may be associated with device memory using the \code{omp_target_associate_ptr()} function.
The function performs a map-enter phase for host memory without allocating
associated storage in device memory.  Instead, the device address of the associated storage is passed
as an argument to the function.
The \code{omp_target_disassociate_ptr()} runtime function
performs a map-exit phase for host memory, but does not free the associated
storage in device memory.  
\index{Accelerators!omp\_target\_associate\_ptr} 
\index{OpenMP runtime functions!omp\_target\_associate\_ptr} 
\index{Accelerators!omp\_target\_disassociate\_ptr} 
\index{OpenMP runtime functions!omp\_target\_disassociate\_ptr} 
\index{Accelerators!omp\_target\_is\_present} 
\index{OpenMP runtime functions!omp\_target\_is\_present} 

%The \code{omp_target_is_present()} function may be
%used to check if a variable is already present in a device data environment.

\begin{figure*}[!tb]
\begin{verbatim}
 1 #include <omp.h>
 2 void stream(float *restrict a, int n, int chunk, int dev)
 3 {
 4   int size = sizeof(float)*chunk;
 5   float *devptr = omp_target_alloc(size, dev);
 6 
 7   for (int i=0; i<n; i+=chunk)
 8   {
 9      omp_target_associate_ptr(&a[i], devptr, size, 0, dev);
10 
11      #pragma omp target map(always,tofrom:a[i:chunk]) device(dev)
12      for (int j=i; j<i+chunk; j++)
13         a[j] = 1/(1+a[j]);
14 
15      omp_target_disassociate_ptr(&a[i], dev);
16   }
17   omp_target_free(devptr, dev);
18 }
\end{verbatim}
\caption{ \textbf {Map host memory to dynamically allocated device memory} -- \small
          Iteratively associate a smaller device memory buffer with a section
          of a larger \texttt{a} buffer.
         }
\label{figure:chapter6-assoc}
\end{figure*}

Figure~\ref{figure:chapter6-assoc} 
shows an example that uses these functions to stream sections of a large
buffer stored in host memory through a smaller buffer allocated in device
memory.


At line $5$, the pointer variable \code{devptr} is assigned the device address of
a \code{chunk}-sized buffer of type float allocated in device memory.
The pointer variable \code{a} points to a buffer of size \code{n} float elements.  
For simplicity, assume that $n \% chunk = 0$ and $n >= chunk$.
The loop starting on line $7$, iteratively maps a \code{chunk}-sized section of
the \code{a} buffer to the \code{devptr} buffer and executes a target region that
operates on the section.

The call to \code{omp_target_associate_ptr()} at line $9$ performs a map-entry
phase for the section, adding it to the device data
environment.  The associated storage for the section is the device memory pointed
to by \code{devptr}.

The section appears in the \code{map} clause as the array section \code{a[i:chunk]} in order to
specify its offset and size in the associated target region at lines $11-13$.

Because the section has a non-zero reference count, the \code{always}
\emph{map-type-modifier} is required to force the map-entry phase assignment of the
section from the host to the device.
Likewise, during the map-exit phase,
the \code{always} \emph{map-type-modifier} ensures that the assignment of the
section from the device to the host occurs.
Note that, instead of the \code{always} \emph{map-type-modifier}, 
\code{target update to(a[i:chunk])} and \code{target update from(a[i:chunk])}
constructs may be used before and after the target region.

The call to \code{omp_target_disassociate()} at line $15$ performs a 
map-exit phase for the section and removes it from
the device data environment but does not free the associated
device memory.
Finally, after the loop completes, the call to \code{omp_target_free()}
at line $17$ frees the \code{devptr} buffer. 

\paragraph{Copy a Sub-matrix Between Two Matrices}

\index{Accelerators!omp\_target\_memcpy\_rect} 
\index{OpenMP runtime functions!omp\_target\_memcpy\_rect} 
An example program is presented below that demonstrates the 
\code{omp_target_memcpy_rect} function. 
The program copies a $4x4$ sub-matrix between two $8x8$ matrices.  
The source matrix is allocated in the device memory of the
default accelerator device, and the destination matrix is in host memory.
The first part of the program is shown in Figure~\ref{figure:chapter6-copy-volume}.

\begin{figure*}[!tb]
\begin{verbatim}
 1 #include <stdio.h>
 2 #include <omp.h>
 3 int copy_2d(void *dst, void *src, int dst_dev, int src_dev,
 4             int sz, int vol_sz, int offset)
 5 {
 6    const int num_dims = 2;
 7    const int vol_dims[2] = {vol_sz, vol_sz};
 8    const int dst_dims[2] = {sz, sz};
 9    const int src_dims[2] = {sz, sz};
10    const int dst_offset[2] = {offset, offset};
11    const int src_offset[2] = {0, 0};
12 
13    return omp_target_memcpy_rect(dst, src, sizeof(char),
14          num_dims,
15          vol_dims,
16          dst_offset, src_offset,
17          dst_dims, src_dims,
18          dst_dev, src_dev);
19 }
\end{verbatim}
\caption{ \textbf {Copy a sub-matrix from a source matrix to a destination matrix - part 1} -- \small
          Two-dimensional square matrices are assumed.
          Copy a sub-matrix from \texttt{src[0][0]} to \texttt{dst[offset][offset]}.
         }
\label{figure:chapter6-copy-volume}
\end{figure*}

The \code{copy\_2d()} function assumes that the matrices and the sub-matrix have two dimensions and are square.  
The arrays initialized at lines $6-9$ define the dimensions of the matrices 
and the sub-matrix.  
The offset arrays initialized at lines $10-11$ are indexes into
the matrices and, along with the \code{dst} and \code{src} pointer variables, are used to determine the starting source
and destination addresses.
The call to the \code{omp_target_memcpy_rect()} function at lines $13-18$ copies a \code{vol\_sz} by \code{vol\_sz}
sub-matrix from the source device address starting at \code{src[0][0]} to the destination host address starting at \code{dst[offset][offset]}.
The second part of the program is shown in Figure~\ref{figure:chapter6-copy-volume-p2}.

\begin{figure*}[!tb]
\begin{verbatim}
21 #define N 8
22 void main()
23 {
24   int dst_dev = omp_get_initial_device();
25   int src_dev = omp_get_default_device();
26   unsigned char DST[N][N];
27   unsigned char (*SRC)[N] = omp_target_alloc(N*N, src_dev);
28 
29   #pragma omp target is_device_ptr(SRC) nowait
30   for (int i=0; i<N; i++)
31     for (int j=0; j<N; j++) SRC[i][j] = 1;
32 
33   for (int i=0; i<N; i++)
34     for (int j=0; j<N; j++) DST[i][j] = 0;
35 
36   #pragma taskwait
37   copy_2d(DST, SRC, dst_dev, src_dev, N, 4, 2);
38 
39   omp_target_free(SRC, src_dev);
40 
41   for (int i=0; i<N; i++) {
42     for (int j=0; j<N; j++) printf("%d" , DST[i][j]);
43     printf("\n");
44   }
45 }
\end{verbatim}
\caption{ \textbf {Copy a sub-matrix from a source matrix to a destination matrix - part 2} -- \small
          Allocate and initialize an $8x8$ \texttt{SRC} matrix on an accelerator and fill it with $1$.
          Initialize an $8x8$ \texttt{DST} matrix on the host and fill it with $0$.
          Copy a $4x4$ sub-matrix from \texttt{SRC[0][0]} to \texttt{DST[2][2]}.
         }
\label{figure:chapter6-copy-volume-p2}
\end{figure*}

The \code{SRC} matrix is dynamically allocated in the default device's memory
by the call at line $27$ to the \code{omp_target_alloc()} function.  
The elements in the $8x8$ \code{SRC} matrix are initialized to $1$ in the target region,
and the elements in the $8x8$ \code{DST} matrix are initialized to $0$ on the host device.

\index{Accelerators!Is\_device\_ptr clause}
\index{OpenMP clauses!is\_device\_ptr}
\index{Accelerators!Nowait clause}
\index{OpenMP clauses!nowait}
\index{Tasking!Taskwait construct}
\index{Accelerators!Target task}
The \code{is_device_ptr} clause is necessary on the \code{target} construct 
at line $29$, because \code{SRC} is a device pointer.
Due to the \code{nowait} clause, the execution of the target region is enclosed
in a deferrable target task, and
the initialization of the \code{SRC} and \code{DST} matrices may execute in parallel.
The host thread cannot continue past the \code{taskwait} construct until the target task
has completed.  After this point, both matrices have been initialized.

The call to the function \code{copy\_2d()} at line $37$ copies a $4x4$ sub-matrix
from the device address starting at \code{SRC[0][0]} to the host address starting at \code{DST[2][2]}.  The call to
\code{omp_target_free} at line $39$ frees the memory allocated for the \code{SRC}
matrix.  Lines $41-43$ print the \code{DST} matrix after the copy has occurred.  The output of the program is
shown in Figure~\ref{figure:chapter6-copy-volume-output}.

\begin{figure*}[!tb]
\begin{verbatim}
00000000
00000000
00111100
00111100
00111100
00111100
00000000
00000000
\end{verbatim}
\caption{ \textbf {Example output from the sub-matrix copy program } -- \small
          This is the output from the program in Figure~\ref{figure:chapter6-copy-volume}
          and Figure~\ref{figure:chapter6-copy-volume-p2}.
          A $4x4$ sub-matrix from the \texttt{SRC} matrix was copied into the center of the \texttt{DST} matrix.
         }
\label{figure:chapter6-copy-volume-output}
\end{figure*}
