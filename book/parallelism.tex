
\chapter{Device parallelism}
\label{chapter:parallelism}

\begin{itemize}
  \item So far learnt how to transfer execution to the device.
  \item Also know how to get some memory to the device.
  \item But not expressed any parallelism on the device yet.
  \item This is the focus here: running in parallel on a target device.
  \item Firstly, we introduce the loop directive, then the hierarchical view of parallelism, with three levels.
  \item This chapter will go through them in detail.
  \item Crucial to remain strictly in OpenMP terminology: the target is still an abstract device (GPU).
  \item We {\bf do not} talk about how OpenMP teams/threads/simd map to GPU specific things like work-items/work-groups/compute units etc. That will come in Chapter~\ref{chapter:portable}.
\end{itemize}

\section{The loop directive}
\begin{itemize}
  \item Introduce the loop directive.
  \item First way of parallelising on the device.
  \item Good for one big parallel loop.
  \item Challenges of the semantics of nested loop directives - suggest to avoid doing it on the target.
  \item The spectrum of prescriptive vs descriptive.
  \item What if you need more control? What happens underneath in OpenMP? Lead into next sections.
\end{itemize}

\section{The goal: teams distribute parallel for simd}
\begin{itemize}
  \item Remind the readers we're heading towards the full directive.
\end{itemize}

\section{Teams}
\begin{itemize}
  \item We know about teams from the host.
  \item Threads are launched in a team.
  \item Allowed to synchronize threads in a team with barrier.
  \item Explain barrier if not already done in Chapter~\ref{chapter:overview}.
  \item May have multiple teams, a league of teams.
  \item num\_teams clause to target directive.
  \item num\_threads clause for threads in a team. Multiplicative factor.
  \item Disclaimer: clauses specifying numbers not good style. See Chapter~\ref{chapter:portable}.
  \item But need to know about multiple teams having one or more threads.
  \item Execution so far is master thread in each team executes the code in the target region.
\end{itemize}

\subsection{teams distribute}
\begin{itemize}
  \item Workshare loop iterations between teams.
  \item Only master thread in each time executes.
  \item dist\_schedule clause.
\end{itemize}

\section{parallel}
\begin{itemize}
  \item Parallelism for threads in each team.
  \item Will apply to all teams.
\end{itemize}

\subsection{parallel for}
\begin{itemize}
  \item Workshare loop iterations between threads in a team.
  \item Interaction with the teams distribute: iterations distributed in chunks by teams, then iterations of chunk workshared between threads.
  \item Combined directive: teams distribute parallel for
\end{itemize}

\section{simd}
\begin{itemize}
  \item Each thread executes SIMD instructions.
  \item Must be applied to a loop.
  \item Compiler will vectorize it.
  \item Be careful, because this doesn't behave the same as worksharing depending on the nesting.
  \item See the Beyond OpenMP book for more details.
\end{itemize}

\section{teams distribute parallel for simd}
\begin{itemize}
  \item Use the combined directive to show parallelism of a big loop.
  \item Compiler and runtime will deal with mapping and distribution of work.
  \item Very similar to loop if you let compiler choose the distribution which you should.
\end{itemize}

\section{Example: Parallel vector add}
\begin{itemize}
  \item Take vector add from previous chapter.
  \item Arrays still on stack (no map clauses).
  \item Keep the data on the stack so donâ€™t have to do the data movement? Means we can do all the data movement together later.
  \item Example shows adding parallelism using teams distribute parallel for simd.
\end{itemize}

\section{Examples: Batched DGEMM}
\begin{itemize}
  \item Shows example of hierarchical parallelism.
  \item Compute one matrix per team, using multiple threads.
  \item Recommend using linear algebra libraries, this is a motivating example.
\end{itemize}

